{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e9eb637",
   "metadata": {},
   "source": [
    "# Assignment 2 - SARSA /Actor Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71864714",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "\n",
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c83e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "rnd = np.random.default_rng(112233)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e840e064",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "## Building Cart Pole ( Q-learning / ACTOR CRITIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9de9b31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00264488, -0.02958185,  0.01406104,  0.01702181], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf15ea05",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "## Building Q-learning class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "895509ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class qlearning:\n",
    "    def __init__(self, env, alpha=.85, gamma=.95, epsilon=.1, bins=10):\n",
    "        self.a = alpha\n",
    "        self.g = gamma\n",
    "        self.q = self.gen_table(env, bins)\n",
    "        self.e = epsilon\n",
    "        self.n_bins = bins\n",
    "\n",
    "        # changing bounds into more compact values to speed up training (fewer bins needed for this accuracy):\n",
    "        self.env_space = [[3, -3],\n",
    "                          [6, -6],\n",
    "                          [0.300, -0.300],\n",
    "                          [5, -5]]\n",
    "\n",
    "        return\n",
    "\n",
    "    def gen_table(self, env, bins):\n",
    "        action_dim = env.action_space.n\n",
    "\n",
    "        table = np.random.uniform(low=-0.001, high=0.001, size=(bins, bins, bins, bins, action_dim))\n",
    "\n",
    "        self.q = table\n",
    "        return self.q\n",
    "\n",
    "    def update(self, reward, state, action, next_state):\n",
    "        a, b, c, d, e = self.get_s(state, action)\n",
    "        a_, b_, c_, d_ = self.get_s(next_state)\n",
    "\n",
    "        self.q[a][b][c][d][e] = self.q[a][b][c][d][e] + self.a * (\n",
    "                reward + self.g * np.max(self.q[a_][b_][c_][d_]) - self.q[a][b][c][d][e])\n",
    "\n",
    "        return None\n",
    "\n",
    "    def choose(self, env, state):\n",
    "\n",
    "        if rnd.random() < self.e:\n",
    "            # random sampling\n",
    "            chosen = rnd.choice(list(range(env.action_space.n)))\n",
    "        else:\n",
    "            # greedy choice\n",
    "            table = self.q\n",
    "            for miniState in self.get_s(state):\n",
    "                table = table[miniState]\n",
    "\n",
    "            chosen = np.argmax(table)\n",
    "        return chosen\n",
    "\n",
    "    def get_s(self, state, action=None):\n",
    "        indexes = []\n",
    "        for i, feature in enumerate(state):\n",
    "            max_value = self.env_space[i][0]\n",
    "            min_value = self.env_space[i][1]\n",
    "\n",
    "            if (feature > max_value) or (feature < min_value):\n",
    "                raise ValueError(\n",
    "                    f\"Feature out of bounds for feature{str(i)} on bins : {str(feature)}  |min : {str(min_value)} - \"\n",
    "                    f\"max :{str(max_value)}|\")\n",
    "            window_size = (max_value - min_value) / self.n_bins\n",
    "            bin_loc = (feature - min_value) // window_size\n",
    "            indexes.append(int(bin_loc))\n",
    "\n",
    "        if action is None:\n",
    "            return indexes\n",
    "        else:\n",
    "            return indexes + [action]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2713ea36",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "## Building the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b697b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining one episode\n",
    "def episode(model, env, render=False, penalty=250):\n",
    "    state = env.reset()[0]\n",
    "    if render:\n",
    "        env.render()\n",
    "    ended = False\n",
    "    ep_reward = 0\n",
    "\n",
    "    while not ended:\n",
    "\n",
    "        action = model.choose(env, state)\n",
    "\n",
    "        # take A from S and get S'\n",
    "        new_state, reward, ended, time_limit, prob = env.step(action)\n",
    "\n",
    "        if ended:\n",
    "            reward -= penalty\n",
    "\n",
    "        model.update(reward, state, action, new_state, final=ended)\n",
    "\n",
    "        # S <- S'\n",
    "        state = new_state\n",
    "        ep_reward += reward\n",
    "        if time_limit:\n",
    "            break\n",
    "\n",
    "    if render:\n",
    "        env.close()\n",
    "    return ep_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "281d21fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining process for each of the runs\n",
    "def run(model, env, episode_n=1000, verbose=True, penalty=250):\n",
    "    run_results = []\n",
    "    for i, mode in enumerate(range(episode_n)):\n",
    "        if verbose and (len(run_results) > 1):\n",
    "            print(f\"\\n{i + 1}th Segment: {np.mean(run_results)} avg reward\", end='')\n",
    "        reward = episode(model, env, penalty=penalty)\n",
    "        run_results.append(reward)\n",
    "\n",
    "    return run_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84f623d",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "\n",
    "## Running the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c47ac449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 10, 10, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configurations\n",
    "n_bins = 10\n",
    "\n",
    "epsilons = [.1, .2, .5]\n",
    "learning_rates = [1 / 4, 1 / 8, 1 / 16]\n",
    "\n",
    "n_runs = 10\n",
    "rolling_window = 10\n",
    "\n",
    "training_size = 10\n",
    "testing_size = 1\n",
    "df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bfe2ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on |Epsilon: 0.01\t| alpha: 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\almei\\AppData\\Local\\Temp\\ipykernel_28952\\844426517.py:20: RuntimeWarning: overflow encountered in float_scalars\n",
      "  window_size = (max_value - min_value) / bins\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_runs):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# creating model copies for each run \u001b[39;00m\n\u001b[0;32m      8\u001b[0m     n_model \u001b[38;5;241m=\u001b[39mqlearning(q_table\u001b[38;5;241m.\u001b[39mcopy(),alpha\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39ma,epsilon\u001b[38;5;241m=\u001b[39mepsilon)\n\u001b[1;32m----> 9\u001b[0m     general_results[model\u001b[38;5;241m.\u001b[39ma][model\u001b[38;5;241m.\u001b[39me][i] \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mn_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_bins\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(model, env, episode_n, verbose, n_bins)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mi \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mth Segment:\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m     run_results[i] \u001b[38;5;241m=\u001b[39m \u001b[43mepisode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_bins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m run_results\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mepisode\u001b[1;34m(model, env, n_bins)\u001b[0m\n\u001b[0;32m      7\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ended:\n\u001b[1;32m---> 11\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# take A from S and get S'\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     new_state, reward, ended, time_limit, prob \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mqlearning.choose\u001b[1;34m(self, env, state)\u001b[0m\n\u001b[0;32m     22\u001b[0m     chosen \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)))\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# greedy choice\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     chosen \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chosen\n",
      "\u001b[1;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "# Runing the training\n",
    "\n",
    "for alpha in learning_rates:\n",
    "    for epsilon in epsilons:\n",
    "        print(f'Training on |Epsilon: {str(epsilon)}\\t| Alpha: {str(alpha)}')\n",
    "\n",
    "        episode_results = []\n",
    "        for i in range(n_runs):\n",
    "            result_df = pd.DataFrame()\n",
    "            # creating model copies for each run\n",
    "            n_model = qlearning(env, alpha=alpha, epsilon=epsilon, bins=n_bins)\n",
    "            result_df['ep_reward'] = run(n_model, env, verbose=False)\n",
    "            result_df['alpha'] = alpha\n",
    "            result_df['epsilon'] = epsilon\n",
    "            result_df['run'] = i\n",
    "            if df is None:\n",
    "                df = result_df.copy()\n",
    "            else:\n",
    "                df = pd.concat([df, result_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebbd7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving dataset if desired\n",
    "# df.to_csv('Qlearning.csv', index=False, sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db345a0b",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "## Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc6b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e6c09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
