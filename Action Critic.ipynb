{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e9eb637",
   "metadata": {},
   "source": [
    "# Assignment 2 - Action Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71864714",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "\n",
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c83e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "rnd = np.random.default_rng(112233)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e840e064",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "## Building Cart Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9de9b31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00264488, -0.02958185,  0.01406104,  0.01702181], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf15ea05",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "## Building Q-learning class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "895509ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class qlearning():\n",
    "    def __init__(self, decision_matrix, alpha=.85, gamma=.95,epsilon=.1):\n",
    "        self.a = alpha\n",
    "        self.g = gamma\n",
    "        self.q = decision_matrix\n",
    "        \n",
    "        self.e = epsilon\n",
    "\n",
    "        return\n",
    "\n",
    "    def update(self, reward, state, action, next_state): \n",
    "        \n",
    "        self.q[state, action] = self.q[state, action] + self.a * (\n",
    "            reward + self.g * np.max(self.q[next_state, :]) - self.q[state, action])\n",
    "\n",
    "        return None\n",
    "\n",
    "    def choose(self, env, state):\n",
    "        \n",
    "        if rnd.random()< self.e:\n",
    "            # random sampling\n",
    "            chosen = rnd.choice(list(range(env.action_space.n)))\n",
    "        else:\n",
    "            # greedy choice\n",
    "            chosen = np.argmax(self.q[state])\n",
    "        return chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef359ae",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "## Building Action critic class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b8e93f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AC():\n",
    "    def __init__(self, decision_matrix, alpha=.85, gamma=.95,epsilon=.1):\n",
    "        self.a = alpha\n",
    "        self.g = gamma\n",
    "        self.q = decision_matrix\n",
    "        self.e = epsilon\n",
    "\n",
    "        return\n",
    "\n",
    "    def update(self, reward, state, action, next_state): \n",
    "        \n",
    "        self.q[state, action] = self.q[state, action] + self.a * (\n",
    "            reward + self.g * np.max(self.q[next_state, :]) - self.q[state, action])\n",
    "\n",
    "        return None\n",
    "\n",
    "    def choose(self, env, state):\n",
    "        \n",
    "        if rnd.random()< self.e:\n",
    "            # random sampling\n",
    "            chosen = rnd.choice(list(range(env.action_space.n)))\n",
    "        else:\n",
    "            # greedy choice\n",
    "            chosen = np.argmax(self.q[state])\n",
    "        return chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "833727c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table generator and converter\n",
    "\n",
    "def gen_table(env, bins=10):\n",
    "    observation_dim = len(env.observation_space.high)\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    table_dim = [bins] * observation_dim + [action_dim] \n",
    "    \n",
    "    table = np.zeros((table_dim))\n",
    "                     \n",
    "    return table\n",
    "    \n",
    "def get_s(state, table, env, bins=10):\n",
    "    result = table\n",
    "    for i, feature in enumerate(state):\n",
    "        max_value = env.observation_space.high[i]\n",
    "        min_value = env.observation_space.low[i]\n",
    "\n",
    "\n",
    "        window_size = (max_value - min_value) / bins\n",
    "        bin_loc = (feature - min_value) // window_size\n",
    "        result = result[int(bin_loc)]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2713ea36",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "## Building the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b697b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining one episode\n",
    "def episode(model, env, n_bins):\n",
    "    state = env.reset()\n",
    "    state = get_s(state[0], model.q, env, n_bins)\n",
    "\n",
    "    ended = False\n",
    "    reward = 0\n",
    "\n",
    "    while not ended:\n",
    "\n",
    "        action = model.choose(env, state)\n",
    "\n",
    "        # take A from S and get S'\n",
    "        new_state, reward, ended, time_limit, prob = env.step(action)\n",
    "        new_state = get_s(new_state, model.q, env, n_bins)\n",
    "\n",
    "        model.update(reward, state, action, new_state)\n",
    "\n",
    "        # S <- S'\n",
    "        state = new_state\n",
    "\n",
    "        if time_limit:\n",
    "            break\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "281d21fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining process for each of the runs\n",
    "def run(model, env, episode_n=1000,verbose=True,n_bins=10):\n",
    "    run_results = {}\n",
    "    for i, mode in enumerate(range(episode_n)):\n",
    "        if verbose:\n",
    "            print(f\"\\n{i + 1}th Segment:\", end='')\n",
    "            \n",
    "        run_results[i] = episode(model, env, n_bins)\n",
    "\n",
    "    return run_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84f623d",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "\n",
    "## Running the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c47ac449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 10, 10, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configurations\n",
    "n_bins = 10\n",
    "\n",
    "epsilons = [.01,.1,.5]\n",
    "learning_rates = [.15,.5,.85]\n",
    "\n",
    "n_runs = 10\n",
    "rolling_window = 10\n",
    "\n",
    "\n",
    "\n",
    "training_size = 10\n",
    "testing_size = 1\n",
    "\n",
    "\n",
    "q_table = gen_table(env,n_bins)\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97f02fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the model\n",
    "\n",
    "models = []\n",
    "\n",
    "general_results = {}\n",
    "for alpha in learning_rates:\n",
    "    general_results[alpha] = {}\n",
    "    for epsilon in epsilons:\n",
    "        general_results[alpha][epsilon] = {}\n",
    "\n",
    "        #creating model to use as standard for each run config\n",
    "        models.append(qlearning(q_table.copy(),alpha=alpha,epsilon=epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bfe2ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on |Epsilon: 0.01\t| alpha: 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\almei\\AppData\\Local\\Temp\\ipykernel_28952\\844426517.py:20: RuntimeWarning: overflow encountered in float_scalars\n",
      "  window_size = (max_value - min_value) / bins\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_runs):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# creating model copies for each run \u001b[39;00m\n\u001b[0;32m      8\u001b[0m     n_model \u001b[38;5;241m=\u001b[39mqlearning(q_table\u001b[38;5;241m.\u001b[39mcopy(),alpha\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39ma,epsilon\u001b[38;5;241m=\u001b[39mepsilon)\n\u001b[1;32m----> 9\u001b[0m     general_results[model\u001b[38;5;241m.\u001b[39ma][model\u001b[38;5;241m.\u001b[39me][i] \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mn_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_bins\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(model, env, episode_n, verbose, n_bins)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mi \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mth Segment:\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m     run_results[i] \u001b[38;5;241m=\u001b[39m \u001b[43mepisode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_bins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m run_results\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mepisode\u001b[1;34m(model, env, n_bins)\u001b[0m\n\u001b[0;32m      7\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ended:\n\u001b[1;32m---> 11\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# take A from S and get S'\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     new_state, reward, ended, time_limit, prob \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mqlearning.choose\u001b[1;34m(self, env, state)\u001b[0m\n\u001b[0;32m     22\u001b[0m     chosen \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)))\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# greedy choice\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     chosen \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chosen\n",
      "\u001b[1;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "# Runing the training\n",
    "\n",
    "for model in models:\n",
    "    print(f'Training on |Epsilon: {str(model.e)}\\t| alpha: {str(model.a)}')\n",
    "    \n",
    "    for i in range(n_runs):\n",
    "        # creating model copies for each run \n",
    "        n_model =qlearning(q_table.copy(),alpha=model.a,epsilon=epsilon)\n",
    "        general_results[model.a][model.e][i] = run(n_model,env,verbose=False,n_bins=n_bins)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75806285",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "## Generating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dbf86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict({(a,b,c,d,e,f): general_results[a][b][c][d][e][f]\n",
    "                                           for a in general_results.keys() \n",
    "                                           for b in general_results[a].keys()\n",
    "                                           for c in general_results[a][b].keys()\n",
    "                                           for d in general_results[a][b][c].keys()\n",
    "                                           for e in general_results[a][b][c][d].keys()\n",
    "                                           for f in general_results[a][b][c][d][e].keys()},\n",
    "                                           orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b37c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df = df.rename(columns={'level_0':'sarsa','level_1':'alpha','level_2':'temperature','level_3':'run','level_4':'segment','level_5':'episode'})\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db345a0b",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "## Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc6b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e6c09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
